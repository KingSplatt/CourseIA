{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19514e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace0497",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m img_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Image-Classification-1/train/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Data Augmentation más agresivo para evitar overfitting\u001b[39;00m\n\u001b[0;32m      7\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      8\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m      9\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuración mejorada\n",
    "# Usar las variables ya definidas en el notebook\n",
    "# img_size, batch_size, dataset, train_datagen, val_datagen ya están definidos\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset + \"/train\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    dataset + \"/train\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo CNN mejorado con regularización\n",
    "def create_improved_model(num_classes):\n",
    "    model = Sequential([\n",
    "        # Primer bloque convolucional\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Segundo bloque convolucional\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Tercer bloque convolucional\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Cuarto bloque convolucional\n",
    "        Conv2D(256, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Clasificador\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear el modelo mejorado\n",
    "improved_model = create_improved_model(train_generator.num_classes)\n",
    "\n",
    "# Compilar con learning rate más bajo\n",
    "improved_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks para evitar overfitting\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo mejorado\n",
    "print(\"Entrenando modelo mejorado...\")\n",
    "history = improved_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=50,  # Más épocas pero con early stopping\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Función mejorada para predicción\n",
    "def predict_emotion_improved(model, img_path, class_labels):\n",
    "    \"\"\"\n",
    "    Predice la emoción de una imagen con confianza\n",
    "    \"\"\"\n",
    "    if not os.path.exists(img_path):\n",
    "        return f\"Imagen no encontrada: {img_path}\"\n",
    "    \n",
    "    # Cargar y preprocesar imagen\n",
    "    img = image.load_img(img_path, target_size=img_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "    \n",
    "    # Predicción\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_idx = np.argmax(predictions, axis=1)[0]\n",
    "    confidence = np.max(predictions) * 100\n",
    "    \n",
    "    # Obtener nombre de la clase\n",
    "    emotion = class_labels[predicted_class_idx]\n",
    "    \n",
    "    # Mostrar todas las probabilidades\n",
    "    print(f\"\\n=== PREDICCIÓN DE EMOCIÓN ===\")\n",
    "    print(f\"Emoción predicha: {emotion}\")\n",
    "    print(f\"Confianza: {confidence:.2f}%\")\n",
    "    print(f\"\\nDistribución de probabilidades:\")\n",
    "    for i, prob in enumerate(predictions[0]):\n",
    "        print(f\"{class_labels[i]}: {prob*100:.2f}%\")\n",
    "    \n",
    "    return emotion, confidence\n",
    "\n",
    "# Ejemplo de uso (descomenta para probar)\n",
    "\"\"\"\n",
    "# Obtener etiquetas de clase\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Probar con una imagen\n",
    "img_path = r\"C:\\IA\\CourseIA\\Unidad4\\Tarea1\\pruebados.jpg\"\n",
    "emotion, confidence = predict_emotion_improved(improved_model, img_path, class_labels)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9976dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo mejorado\n",
    "def evaluate_model_performance(model, validation_generator):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento del modelo\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EVALUACIÓN DEL MODELO ===\")\n",
    "    \n",
    "    # Evaluación en conjunto de validación\n",
    "    val_loss, val_accuracy = model.evaluate(validation_generator, verbose=0)\n",
    "    print(f\"Precisión en validación: {val_accuracy:.4f}\")\n",
    "    print(f\"Pérdida en validación: {val_loss:.4f}\")\n",
    "    \n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Guardar el modelo mejorado\n",
    "def save_improved_model(model, filename=\"emotion_recognition_improved.h5\"):\n",
    "    \"\"\"\n",
    "    Guarda el modelo entrenado\n",
    "    \"\"\"\n",
    "    model.save(filename)\n",
    "    print(f\"Modelo guardado como: {filename}\")\n",
    "\n",
    "# Función para visualizar el historial de entrenamiento\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Visualiza las métricas de entrenamiento\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Precisión\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Pérdida\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Modelo mejorado creado exitosamente!\")\n",
    "print(\"Características principales:\")\n",
    "print(\"- Data augmentation para evitar overfitting\")\n",
    "print(\"- Arquitectura más profunda con regularización\")\n",
    "print(\"- Batch normalization y dropout\")\n",
    "print(\"- Early stopping y reducción de learning rate\")\n",
    "print(\"- Evaluación detallada del rendimiento\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
